import os
import json
import re
import base64
from io import BytesIO
from typing import Dict, Any, List, Optional, Tuple

import streamlit as st
import yaml
import altair as alt

# --- Optional PDF / OCR libraries ---
try:
    from PyPDF2 import PdfReader
except ImportError:
    PdfReader = None

try:
    from pdf2image import convert_from_bytes
except ImportError:
    convert_from_bytes = None

try:
    import pytesseract
except ImportError:
    pytesseract = None

# --- LLM client libraries ---
from openai import OpenAI
import google.generativeai as genai
from anthropic import Anthropic

# --- xAI (Grok) SDK (per official sample) ---
from xai_sdk import Client as XaiClient
from xai_sdk.chat import user as xai_user, system as xai_system
# from xai_sdk.chat import image as xai_image  # for future image OCR use

# -----------------------------------------------------------
# Nordic Theme + Flower Styles Configuration
# -----------------------------------------------------------

# Light/dark base palettes in a clean Nordic style
FDA_THEMES = {
    "light": {
        "primary": "#2F4F4F",   # dark slate
        "secondary": "#7EA7A6", # muted teal
        "background": "#F5F7F8",# very light grey
        "text": "#1E2A32",      # deep grey-blue
        "accent": "#FF7F50",    # coral for key highlights
    },
    "dark": {
        "primary": "#E0F2F1",   # light teal
        "secondary": "#90A4AE", # blue-grey
        "background": "#111827",# near-black blue-grey
        "text": "#E5E7EB",      # light grey
        "accent": "#FF7F50",    # coral for key highlights
    }
}

# 20 flower-based styles â€“ used as an extra accent layer via "magic wheel"
FLOWER_STYLES = {
    "Lily":       {"icon": "ğŸ¤", "color": "#F6E9E9", "description": "Calm white lily, minimal and pure."},
    "Rose":       {"icon": "ğŸŒ¹", "color": "#F28B82", "description": "Soft rose warmth with subtle contrast."},
    "Tulip":      {"icon": "ğŸŒ·", "color": "#F9B4AB", "description": "Gentle tulip gradients, modern yet cozy."},
    "Lotus":      {"icon": "ğŸª·", "color": "#C1E3E1", "description": "Lotus serenity, aqua pastel balance."},
    "Lavender":   {"icon": "ğŸ’œ", "color": "#C4B5FD", "description": "Lavender mist, calm regulatory focus."},
    "Peony":      {"icon": "ğŸŒ¸", "color": "#FAD4E1", "description": "Peony blush, soft and inviting."},
    "Sunflower":  {"icon": "ğŸŒ»", "color": "#FDE68A", "description": "Bright sunflower, optimistic review mood."},
    "Camellia":   {"icon": "ğŸŒº", "color": "#FBB6CE", "description": "Camellia pink, elegant and clear."},
    "Daisy":      {"icon": "ğŸŒ¼", "color": "#FFF7CC", "description": "Daisy light, high readability."},
    "Hydrangea":  {"icon": "ğŸ©µ", "color": "#BFDBFE", "description": "Hydrangea blue, cool and analytical."},
    "Orchid":     {"icon": "ğŸª»", "color": "#E9D5FF", "description": "Orchid lilac, refined clinical tone."},
    "Magnolia":   {"icon": "ğŸ¤", "color": "#F3E8FF", "description": "Magnolia white-violet, calm authority."},
    "Iris":       {"icon": "ğŸª»", "color": "#A5B4FC", "description": "Iris blue-violet, sharp and focused."},
    "Poppy":      {"icon": "ğŸŒº", "color": "#FDBA74", "description": "Poppy orange, clear highlights."},
    "Anemone":    {"icon": "ğŸŒ¸", "color": "#FDE2E4", "description": "Anemone blush, soft contrast."},
    "Cornflower": {"icon": "ğŸ’ ", "color": "#BFDBFE", "description": "Cornflower blue, structured clarity."},
    "Heather":    {"icon": "ğŸ’œ", "color": "#E5E7EB", "description": "Heather grey-lilac, understated calm."},
    "Edelweiss":  {"icon": "ğŸ¤", "color": "#E5F0FF", "description": "Edelweiss alpine white-blue, Nordic crisp."},
    "Marigold":   {"icon": "ğŸ§¡", "color": "#FED7AA", "description": "Marigold apricot, gentle emphasis."},
    "Bluebell":   {"icon": "ğŸ””", "color": "#C7D2FE", "description": "Bluebell periwinkle, quiet confidence."},
}

REVIEW_CONTEXT_STYLES = {
    "General 510(k)": {
        "icon": "ğŸ“",
        "description": "ä¸€èˆ¬ 510(k) å‚³çµ±é†«ç™‚å™¨æå¯©æŸ¥æƒ…å¢ƒ",
        "color": "#2B6CB0",
    },
    "Orthopedic": {
        "icon": "ğŸ¦´",
        "description": "éª¨ç§‘æ¤å…¥ç‰©èˆ‡å™¨æå¯©æŸ¥æƒ…å¢ƒ",
        "color": "#805AD5",
    },
    "Cardiovascular": {
        "icon": "â¤ï¸",
        "description": "å¿ƒè¡€ç®¡è£ç½®èˆ‡æ”¯æ¶å¯©æŸ¥æƒ…å¢ƒ",
        "color": "#E53E3E",
    },
    "Radiology": {
        "icon": "ğŸ©»",
        "description": "å½±åƒè¨ºæ–·è¨­å‚™èˆ‡ AI è®€ç‰‡è¼”åŠ©å¯©æŸ¥æƒ…å¢ƒ",
        "color": "#3182CE",
    },
    "In Vitro Diagnostic": {
        "icon": "ğŸ§ª",
        "description": "é«”å¤–è¨ºæ–· (IVD) è©¦åŠ‘èˆ‡å„€å™¨å¯©æŸ¥æƒ…å¢ƒ",
        "color": "#38A169",
    },
    "Digital Health": {
        "icon": "ğŸ“±",
        "description": "æ•¸ä½å¥åº·ã€SaMD èˆ‡é è·ç›£æ¸¬ç³»çµ±å¯©æŸ¥æƒ…å¢ƒ",
        "color": "#D53F8C",
    },
    "Surgical": {
        "icon": "ğŸ”ª",
        "description": "æ‰‹è¡“å™¨æ¢°èˆ‡èƒ½é‡è¨­å‚™å¯©æŸ¥æƒ…å¢ƒ",
        "color": "#DD6B20",
    },
    "Dental": {
        "icon": "ğŸ¦·",
        "description": "ç‰™ç§‘è£ç½®èˆ‡ææ–™å¯©æŸ¥æƒ…å¢ƒ",
        "color": "#319795",
    },
    "Anesthesiology": {
        "icon": "ğŸ’¤",
        "description": "éº»é†‰èˆ‡å‘¼å¸æ²»ç™‚è¨­å‚™å¯©æŸ¥æƒ…å¢ƒ",
        "color": "#4A5568",
    },
    "Combination Product": {
        "icon": "ğŸ’Š",
        "description": "è—¥æ¢°çµ„åˆç”¢å“èˆ‡é‚Šç•Œç”¢å“å¯©æŸ¥æƒ…å¢ƒ",
        "color": "#B83280",
    },
}

TRANSLATIONS = {
    "en": {
        "title": "FDA 510(k) Multi-Agent Review Studio",
        "subtitle": "Role: Professional Regulatory AI Orchestrator",
        "theme": "UI Theme",
        "language": "Language",
        "art_style": "Review Context Style",
        "health": "Compliance Health",
        "mana": "AI Resource Capacity",
        "experience": "Case Experience",
        "api_keys": "API Keys",
        "input": "Case Inputs",
        "pipeline": "Review Pipelines",
        "smart_replace": "Smart Editing",
        "notes": "AI Note Keeper",
        "dashboard": "Dashboard",
        "run": "Run Pipeline",
        "level": "Maturity Level",
        "quest_log": "Case Log",
        "achievements": "Milestones",
        "ocr": "Submission OCR Studio",
    },
    "zh": {
        "title": "FDA 510(k) å¤šä»£ç†å¯©æŸ¥å·¥ä½œå®¤",
        "subtitle": "å°ˆæ¥­è§’è‰²ï¼šFDA é†«ç™‚å™¨æ 510(k) å¯©æŸ¥å”ä½œä»£ç†ç³»çµ±",
        "theme": "ä»‹é¢ä¸»é¡Œ",
        "language": "èªè¨€",
        "art_style": "å¯©æŸ¥æƒ…å¢ƒé¢¨æ ¼",
        "health": "åˆè¦å¥åº·åº¦",
        "mana": "AI è³‡æºå®¹é‡",
        "experience": "æ¡ˆä»¶ç¶“é©—å€¼",
        "api_keys": "API é‡‘é‘°",
        "input": "æ¡ˆä»¶è¼¸å…¥",
        "pipeline": "å¯©æŸ¥æµç¨‹",
        "smart_replace": "æ™ºèƒ½ç·¨è¼¯",
        "notes": "AI ç­†è¨˜åŠ©æ‰‹",
        "dashboard": "å„€è¡¨æ¿",
        "run": "åŸ·è¡Œæµç¨‹",
        "level": "å¯©æŸ¥æˆç†Ÿåº¦ç­‰ç´š",
        "quest_log": "æ¡ˆä»¶ç´€éŒ„",
        "achievements": "é‡è¦é‡Œç¨‹ç¢‘",
        "ocr": "é€ä»¶ OCR å·¥ä½œå®¤",
    }
}

# -----------------------------------------------------------
# Session State Initialization
# -----------------------------------------------------------

def init_session_state():
    """Initialize all session state variables"""
    defaults = {
        "theme": "dark",
        "language": "zh",
        "art_style": "General 510(k)",
        "flower_style": "Edelweiss",
        "player_level": 1,
        "health": 100,
        "mana": 100,
        "experience": 0,
        "quests_completed": 0,
        "achievements": [],
        "combat_log": [],
        "template": "## æ¡ˆä»¶æ¨¡æ¿\n\nåœ¨æ­¤æ’°å¯«æˆ–è²¼ä¸Š 510(k) æ¡ˆä»¶ç›¸é—œæ¨¡æ¿å…§å®¹...",
        "observations": "åœ¨æ­¤æ–°å¢è‡¨åºŠã€é¢¨éšªæˆ–æŠ€è¡“è§€å¯Ÿå‚™è¨»...",
        "pipeline_history": [],
        "note_raw_text": "",
        "note_markdown": "",
        "note_formatted": "",
        "note_keywords_output": "",
        "note_entities_json_data": [],
        "note_mindmap_json_text": "",
        "note_wordgraph_json_text": "",
        "note_chat_history": [],
        # OCR Studio state
        "ocr_files": [],              # list of per-file dicts
        "ocr_global_keywords": "510(k), substantial equivalence, risk, performance testing, adverse event, indication, predicate device, è‡¨åºŠ, é¢¨éšª, æ€§èƒ½æ¸¬è©¦, é©æ‡‰ç—‡",
        "combined_markdown": "",
        "combined_entities": [],
        "combined_qa_history": [],
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

# -----------------------------------------------------------
# Utility Functions
# -----------------------------------------------------------

@st.cache_data
def load_agents_config(path: str = "agents.yaml") -> Dict[str, Any]:
    """Load agents configuration from YAML file"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        return {"agents": [], "pipelines": []}

def get_translation(key: str) -> str:
    """Get translated text based on current language"""
    lang = st.session_state.get("language", "zh")
    return TRANSLATIONS.get(lang, TRANSLATIONS["zh"]).get(key, key)

def apply_custom_css():
    """Apply Nordic-style custom CSS with flower accents"""
    theme_key = st.session_state.get("theme", "dark")
    style_key = st.session_state.get("art_style", "General 510(k)")
    flower_key = st.session_state.get("flower_style", "Edelweiss")

    colors = FDA_THEMES[theme_key]
    context_color = REVIEW_CONTEXT_STYLES.get(
        style_key, REVIEW_CONTEXT_STYLES["General 510(k)"]
    )["color"]
    flower_style = FLOWER_STYLES.get(flower_key, FLOWER_STYLES["Edelweiss"])
    flower_color = flower_style["color"]

    # Accent is based on flower + coral highlight
    accent_color = flower_color
    coral = colors["accent"]

    css = f"""
    <style>
    /* Main app container */
    .stApp {{
        background: radial-gradient(circle at top left, {flower_color} 0, {colors['background']} 50%);
        color: {colors['text']};
        font-family: -apple-system, BlinkMacSystemFont, "SF Pro Text", "Segoe UI", system-ui, sans-serif;
    }}

    /* Headers with subtle underline accent */
    h1, h2, h3 {{
        color: {colors['primary']};
        border-bottom: 2px solid rgba(148, 163, 184, 0.35);
        padding-bottom: 4px;
        letter-spacing: 0.02em;
    }}

    /* Buttons */
    .stButton > button {{
        background: linear-gradient(135deg, {accent_color}, {context_color});
        color: #111827;
        border-radius: 999px;
        padding: 0.5rem 1.2rem;
        border: 1px solid rgba(15, 23, 42, 0.12);
        font-weight: 600;
        box-shadow: 0 6px 14px rgba(15, 23, 42, 0.12);
        transition: all 0.18s ease-out;
    }}
    .stButton > button:hover {{
        transform: translateY(-1px);
        box-shadow: 0 10px 24px rgba(15, 23, 42, 0.22);
        opacity: 0.96;
    }}

    /* Status bar container */
    .status-bar {{
        background: linear-gradient(90deg, rgba(15,23,42,0.06), transparent);
        border-radius: 999px;
        padding: 0.25rem 0.6rem;
        margin: 0.25rem 0;
    }}

    /* Card style */
    .review-card {{
        background: rgba(255, 255, 255, 0.75);
        backdrop-filter: blur(10px);
        border-radius: 18px;
        padding: 14px 18px;
        margin: 6px 0;
        border: 1px solid rgba(148, 163, 184, 0.35);
        box-shadow: 0 14px 30px rgba(15, 23, 42, 0.16);
    }}

    /* Tabs */
    .stTabs [data-baseweb="tab-list"] {{
        gap: 0.25rem;
        background-color: rgba(15,23,42,0.04);
        border-radius: 999px;
        padding: 0.2rem;
    }}
    .stTabs [data-baseweb="tab"] {{
        border-radius: 999px;
        font-weight: 600;
        border: none;
    }}
    .stTabs [aria-selected="true"] {{
        background: linear-gradient(135deg, {accent_color}, {context_color});
        color: #111827;
    }}

    /* Input fields */
    .stTextInput > div > div > input,
    .stTextArea > div > div > textarea {{
        background-color: rgba(15,23,42,0.02);
        border-radius: 0.75rem;
        border: 1px solid rgba(148, 163, 184, 0.4);
    }}

    /* Sidebar */
    section[data-testid="stSidebar"] {{
        background: linear-gradient(180deg, rgba(15,23,42,0.92), rgba(15,23,42,0.98));
        color: #E5E7EB !important;
        border-right: 1px solid rgba(148, 163, 184, 0.4);
    }}
    section[data-testid="stSidebar"] * {{
        color: #E5E7EB !important;
    }}

    /* Progress bars */
    .stProgress > div > div > div > div {{
        background: linear-gradient(90deg, {accent_color}, {context_color});
    }}

    /* Expanders */
    .streamlit-expanderHeader {{
        background: rgba(15,23,42,0.2);
        color: #E5E7EB;
        border-radius: 999px;
        font-weight: 600;
    }}

    /* Coral keyword highlight demo */
    .coral-keyword {{
        color: {coral};
        font-weight: 600;
    }}
    </style>
    """
    st.markdown(css, unsafe_allow_html=True)

def update_player_stats(action: str):
    """
    Update abstracted 'player' stats, re-interpreted as review metrics:
    - level: å¯©æŸ¥æˆç†Ÿåº¦ç­‰ç´š
    - health: åˆè¦å¥åº·åº¦
    - mana: AI è³‡æºå®¹é‡
    """
    if action == "quest_complete":
        st.session_state.experience += 10
        st.session_state.quests_completed += 1
        if st.session_state.experience >= st.session_state.player_level * 50:
            st.session_state.player_level += 1
            st.session_state.experience = 0
            st.toast(f"ğŸ¯ å¯©æŸ¥æˆç†Ÿåº¦æå‡ï¼ç›®å‰ç­‰ç´šï¼š{st.session_state.player_level}")
    elif action == "use_mana":
        st.session_state.mana = max(0, st.session_state.mana - 20)
    elif action == "regenerate":
        st.session_state.mana = min(100, st.session_state.mana + 10)
        st.session_state.health = min(100, st.session_state.health + 5)

def add_combat_log(message: str, message_type: str = "info"):
    """Add entry to review activity log"""
    icons = {
        "info": "â„¹ï¸",
        "success": "âœ…",
        "warning": "âš ï¸",
        "error": "âŒ",
        "spell": "ğŸ§ ",
    }
    log_entry = {
        "icon": icons.get(message_type, "â„¹ï¸"),
        "message": message,
        "timestamp": st.session_state.get("quests_completed", 0),
    }
    if "combat_log" not in st.session_state:
        st.session_state.combat_log = []
    st.session_state.combat_log.append(log_entry)
    if len(st.session_state.combat_log) > 200:
        st.session_state.combat_log.pop(0)

# -----------------------------------------------------------
# API Key Management
# -----------------------------------------------------------

def get_api_key_from_env_or_ui(
    provider_name: str,
    env_var: str,
    session_key: str,
    label: str,
) -> Optional[str]:
    """Get API key from environment or user input (do not echo env key)"""
    env_val = os.getenv(env_var)
    if env_val:
        st.caption(f"ğŸ”‘ {label}: å·²å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥")
        st.session_state[session_key] = env_val
        return env_val

    key = st.text_input(
        label,
        value=st.session_state.get(session_key, ""),
        type="password",
    )
    if key:
        st.session_state[session_key] = key
        st.caption(f"ğŸ”‘ {label} å·²æš«å­˜æ–¼å·¥ä½œéšæ®µ")
        return key
    return None

# -----------------------------------------------------------
# LLM Call Router (OpenAI, Gemini, Grok via xai_sdk, Anthropic)
# -----------------------------------------------------------

def call_llm(
    provider: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    max_tokens: int = 512,
    temperature: float = 0.7,
) -> str:
    """Route LLM calls to appropriate provider"""
    provider = provider.lower().strip()

    add_combat_log(f"å‘¼å« {provider} æ¨¡å‹ï¼š{model}", "spell")
    update_player_stats("use_mana")

    if provider == "openai":
        api_key = st.session_state.get("openai_api_key")
        if not api_key:
            raise RuntimeError("OpenAI API key is not set.")
        client = OpenAI(api_key=api_key)
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return resp.choices[0].message.content

    elif provider == "gemini":
        api_key = st.session_state.get("gemini_api_key")
        if not api_key:
            raise RuntimeError("Gemini API key is not set.")
        genai.configure(api_key=api_key)
        model_obj = genai.GenerativeModel(model)
        resp = model_obj.generate_content(
            system_prompt + "\n\nUSER MESSAGE:\n" + user_prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature,
            )
        )
        return resp.text

    elif provider == "xai":
        # Grok via xai_sdk (per official sample)
        api_key = st.session_state.get("xai_api_key")
        if not api_key:
            raise RuntimeError("xAI (Grok) API key is not set.")
        client = XaiClient(api_key=api_key, timeout=3600)
        chat = client.chat.create(model=model)
        chat.append(xai_system(system_prompt))
        chat.append(xai_user(user_prompt))
        response = chat.sample()
        # response.content is typically a string
        return getattr(response, "content", str(response))

    elif provider == "anthropic":
        api_key = st.session_state.get("anthropic_api_key")
        if not api_key:
            raise RuntimeError("Anthropic API key is not set.")
        client = Anthropic(api_key=api_key)
        resp = client.messages.create(
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            system=system_prompt,
            messages=[{"role": "user", "content": user_prompt}],
        )
        if resp.content and len(resp.content) > 0:
            block = resp.content[0]
            if hasattr(block, "text"):
                return block.text
        return json.dumps(resp.model_dump(), indent=2)

    else:
        raise ValueError(f"Unsupported provider: {provider}")

def run_agent(
    agent_cfg: Dict[str, Any],
    user_prompt: str,
    override_provider: Optional[str] = None,
    override_model: Optional[str] = None,
    override_system_prompt: Optional[str] = None,
    max_tokens: int = 512,
    temperature: float = 0.7,
) -> str:
    """Run a single configured agent"""
    provider = override_provider or agent_cfg.get("provider", "openai")
    model = override_model or agent_cfg.get("default_model", "gpt-4o-mini")
    system_prompt = override_system_prompt or agent_cfg.get("system_prompt", "")
    return call_llm(
        provider=provider,
        model=model,
        system_prompt=system_prompt,
        user_prompt=user_prompt,
        max_tokens=max_tokens,
        temperature=temperature,
    )

# -----------------------------------------------------------
# Status Indicators
# -----------------------------------------------------------

def render_status_indicators():
    """Render review status indicators (WOW gauges)"""
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.markdown(f"**{get_translation('level')}**")
        st.metric(label="", value=st.session_state.player_level)

    with col2:
        st.markdown(f"**{get_translation('health')}**")
        st.progress(st.session_state.health / 100)
        st.caption(f"{st.session_state.health}/100")

    with col3:
        st.markdown(f"**{get_translation('mana')}**")
        st.progress(st.session_state.mana / 100)
        st.caption(f"{st.session_state.mana}/100")

    with col4:
        st.markdown(f"**{get_translation('experience')}**")
        max_xp = st.session_state.player_level * 50
        st.progress(st.session_state.experience / max_xp)
        st.caption(f"{st.session_state.experience}/{max_xp}")

def render_activity_log():
    """Render review activity log"""
    st.markdown("### ğŸ“‘ æ´»å‹•ç´€éŒ„")
    with st.expander("æª¢è¦–è¿‘æœŸå‹•ä½œ", expanded=False):
        if st.session_state.combat_log:
            for entry in reversed(st.session_state.combat_log[-40:]):
                st.markdown(f"{entry['icon']} {entry['message']}")
        else:
            st.info("ç›®å‰å°šç„¡æ´»å‹•ç´€éŒ„")

# -----------------------------------------------------------
# Review Context Selector
# -----------------------------------------------------------

def render_review_context_selector():
    """Render interactive review context selector"""
    st.markdown("### ğŸ¥ å¯©æŸ¥æƒ…å¢ƒé¸æ“‡å™¨")

    cols = st.columns(5)
    styles = list(REVIEW_CONTEXT_STYLES.keys())

    for idx, style in enumerate(styles):
        with cols[idx % 5]:
            style_data = REVIEW_CONTEXT_STYLES[style]
            button_label = f"{style_data['icon']} {style}"
            if st.button(
                button_label,
                key=f"style_{style}",
                help=style_data["description"],
                use_container_width=True
            ):
                st.session_state.art_style = style
                add_combat_log(f"åˆ‡æ›å¯©æŸ¥æƒ…å¢ƒç‚ºï¼š{style}", "success")
                st.rerun()

    current_style = st.session_state.get("art_style", "General 510(k)")
    style_data = REVIEW_CONTEXT_STYLES[current_style]
    st.markdown(
        f"<div class='review-card' style='text-align: center; "
        f"background: linear-gradient(135deg, {style_data['color']}33, transparent);'>"
        f"<h3>{style_data['icon']} ç›®å‰æƒ…å¢ƒï¼š{current_style}</h3>"
        f"<p>{style_data['description']}</p>"
        f"</div>",
        unsafe_allow_html=True
    )

# -----------------------------------------------------------
# Enhanced Sidebar (incl. Magic Flower Wheel)
# -----------------------------------------------------------

def render_enhanced_sidebar(config: Dict[str, Any]):
    """Render Nordic-themed sidebar with controls"""
    st.sidebar.markdown(f"# {get_translation('title')}")
    st.sidebar.markdown(f"*{get_translation('subtitle')}*")
    st.sidebar.markdown("---")

    # Theme and Language Selection
    col1, col2 = st.sidebar.columns(2)
    with col1:
        theme = st.selectbox(
            get_translation("theme"),
            ["light", "dark"],
            index=1 if st.session_state.theme == "dark" else 0,
            key="theme_selector"
        )
        if theme != st.session_state.theme:
            st.session_state.theme = theme
            st.rerun()

    with col2:
        lang = st.selectbox(
            get_translation("language"),
            ["zh", "en"],
            index=0 if st.session_state.language == "zh" else 1,
            key="lang_selector"
        )
        if lang != st.session_state.language:
            st.session_state.language = lang
            st.rerun()

    # Magic Flower Wheel
    st.sidebar.markdown("### ğŸŒ¸ Magic Flower Wheel")
    flower_names = list(FLOWER_STYLES.keys())
    flower_labels = [
        f"{FLOWER_STYLES[name]['icon']} {name}" for name in flower_names
    ]
    current_index = flower_names.index(st.session_state.get("flower_style", "Edelweiss"))
    idx = st.sidebar.select_slider(
        "é¸æ“‡ UI èŠ±å‰é¢¨æ ¼",
        options=list(range(len(flower_names))),
        value=current_index,
        format_func=lambda i: flower_labels[i],
        key="flower_wheel",
    )
    chosen_flower = flower_names[idx]
    if chosen_flower != st.session_state.flower_style:
        st.session_state.flower_style = chosen_flower
        add_combat_log(f"åˆ‡æ›èŠ±å‰é¢¨æ ¼ç‚ºï¼š{chosen_flower}", "info")
        st.rerun()

    st.sidebar.markdown("---")

    # Review Status
    st.sidebar.markdown("### ğŸ“Š å¯©æŸ¥ç‹€æ…‹ç¸½è¦½")
    render_status_indicators()
    st.sidebar.markdown("---")

    # API Keys
    st.sidebar.markdown(f"### ğŸ”‘ {get_translation('api_keys')}")
    with st.sidebar.expander("è¨­å®š API é‡‘é‘°"):
        get_api_key_from_env_or_ui(
            "OpenAI", "OPENAI_API_KEY", "openai_api_key", "OpenAI API Key"
        )
        get_api_key_from_env_or_ui(
            "Gemini", "GEMINI_API_KEY", "gemini_api_key", "Gemini API Key"
        )
        get_api_key_from_env_or_ui(
            "xAI", "XAI_API_KEY", "xai_api_key", "xAI (Grok) API Key"
        )
        get_api_key_from_env_or_ui(
            "Anthropic", "ANTHROPIC_API_KEY", "anthropic_api_key", "Anthropic API Key"
        )

    st.sidebar.markdown("---")

    # Model Settings
    st.sidebar.markdown("### âš™ï¸ æ¨¡å‹å‘¼å«è¨­å®š")

    provider = st.sidebar.selectbox(
        "æ¨¡å‹ä¾›æ‡‰å•†",
        ["openai", "gemini", "xai", "anthropic"],
        key="default_provider",
    )

    provider_models = {
        "openai": ["gpt-5-nano", "gpt-4o-mini", "gpt-4.1-mini"],
        "gemini": ["gemini-2.5-flash", "gemini-2.5-flash-lite"],
        "xai": ["grok-4-fast-reasoning", "grok-3-mini"],
        "anthropic": ["claude-3-5-sonnet-latest", "claude-3-opus-latest"],
    }

    st.sidebar.selectbox(
        "æ¨¡å‹ç‰ˆæœ¬",
        provider_models[provider],
        key="default_model",
    )

    st.sidebar.slider(
        "æœ€å¤§è¼¸å‡º Token æ•¸",
        64, 4096, 1024, 64,
        key="default_max_tokens",
    )

    st.sidebar.slider(
        "æº«åº¦ï¼ˆéš¨æ©Ÿæ€§ï¼‰",
        0.0, 1.0, 0.7, 0.05,
        key="default_temperature",
    )

    st.sidebar.markdown("---")

    # Case Log
    st.sidebar.markdown(f"### ğŸ“ {get_translation('quest_log')}")
    st.sidebar.metric("å·²å®Œæˆæ¡ˆä»¶æ•¸", st.session_state.quests_completed)

    if st.sidebar.button("ğŸ”„ æ¢å¾©è³‡æº"):
        update_player_stats("regenerate")
        add_combat_log("AI è³‡æºèˆ‡åˆè¦å¥åº·åº¦å·²é©åº¦æ¢å¾©", "success")
        st.rerun()

# -----------------------------------------------------------
# Input Tab
# -----------------------------------------------------------

def render_input_tab():
    """Render case input tab"""
    st.markdown(f"## ğŸ“ {get_translation('input')}")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.text_area(
            "ğŸ“„ 510(k) æ¡ˆä»¶æ¨¡æ¿ / ä¸»è¦å…§å®¹",
            key="template",
            height=260,
            help="ä¾‹å¦‚ï¼šè¨­å‚™æè¿°ã€é©æ‡‰ç—‡èªªæ˜ã€å¯¦è³ªç­‰åŒæ€§æ¯”è¼ƒã€é¢¨éšªç®¡ç†æ‘˜è¦ç­‰"
        )

        st.text_area(
            "ğŸ” å¯©æŸ¥è§€å¯Ÿèˆ‡å‚™è¨»",
            key="observations",
            height=260,
            help="è¨˜éŒ„å¯©æŸ¥æ­·ç¨‹ä¸­çš„ç–‘å•ã€é¢¨éšªé»ã€éœ€è¿½å•ä¹‹è³‡æ–™ç­‰"
        )

    with col2:
        render_activity_log()

        st.markdown("### âš¡ å¿«é€Ÿå‹•ä½œ")
        if st.button("ğŸ’¾ å„²å­˜ç•¶å‰è¼¸å…¥", use_container_width=True):
            add_combat_log("ç›®å‰æ¡ˆä»¶è¼¸å…¥å·²å„²å­˜ï¼ˆæš«å­˜æ–¼ sessionï¼‰", "success")
            st.success("å·²æš«å­˜ç›®å‰å…§å®¹ã€‚")

        if st.button("ğŸ§¹ æ¸…ç©ºæ¬„ä½", use_container_width=True):
            st.session_state.template = ""
            st.session_state.observations = ""
            add_combat_log("æ¡ˆä»¶è¼¸å…¥æ¬„ä½å·²æ¸…ç©º", "info")
            st.rerun()

# -----------------------------------------------------------
# Pipeline Tab
# -----------------------------------------------------------

def render_pipeline_tab(config: Dict[str, Any]):
    """Render multi-agent 510(k) review pipeline tab"""
    st.markdown(f"## ğŸ”„ {get_translation('pipeline')}")

    if not config or "pipelines" not in config:
        st.warning("âš ï¸ agents.yaml ä¸­æœªæ‰¾åˆ°ä»»ä½•å¯©æŸ¥æµç¨‹ (pipelines) è¨­å®šã€‚")
        return

    col1, col2 = st.columns([2, 1])

    with col1:
        pipeline_options = {p["name"]: p for p in config["pipelines"]}
        selected_name = st.selectbox("ğŸ” é¸æ“‡å¯©æŸ¥æµç¨‹", list(pipeline_options.keys()))
        pipeline = pipeline_options[selected_name]

        st.markdown(f"**æµç¨‹ IDï¼š** `{pipeline['id']}`")
        st.markdown(f"**èªªæ˜ï¼š** {pipeline.get('description', '')}")

        st.markdown("### ğŸ“‚ æµç¨‹æ­¥é©Ÿ")
        for idx, step in enumerate(pipeline["steps"], start=1):
            st.markdown(f"- ç¬¬ {idx} æ­¥ï¼š`{step['agent_id']}`")

        st.markdown("---")

        override_prompt = st.text_area(
            "ğŸ“Œ å…¶ä»–è£œå……èªªæ˜ / ç‰¹åˆ¥æŒ‡ç¤º",
            "ä¾‹å¦‚ï¼šæ­¤æ¡ˆä»¶é¢¨éšªåé«˜ï¼Œè«‹æé«˜é¢¨éšªè©•ä¼°èˆ‡æ³•è¦æ¯”å°çš„åš´è¬¹åº¦ã€‚",
            height=120,
        )

        col_a, col_b = st.columns(2)
        with col_a:
            provider = st.selectbox(
                "æ¨¡å‹ä¾›æ‡‰å•†è¦†å¯«ï¼ˆé¸å¡«ï¼‰",
                ["(ä½¿ç”¨é è¨­)", "openai", "gemini", "xai", "anthropic"],
            )
        with col_b:
            model_override = st.text_input("æ¨¡å‹åç¨±è¦†å¯«ï¼ˆé¸å¡«ï¼‰", "")

        if st.button(f"â–¶ï¸ {get_translation('run')}", use_container_width=True):
            if st.session_state.mana < 20:
                st.error("âŒ AI è³‡æºä¸è¶³ï¼Œè«‹å…ˆæŒ‰å·¦å´ã€æ¢å¾©è³‡æºã€ã€‚")
                return

            template = st.session_state.get("template", "")
            observations = st.session_state.get("observations", "")
            current_input = (
                "ã€510(k) æ¡ˆä»¶è¼¸å…¥ã€‘\n"
                f"{template}\n\n"
                "ã€å¯©æŸ¥è§€å¯Ÿèˆ‡å‚™è¨»ã€‘\n"
                f"{observations}\n\n"
                "ã€é¡å¤–æŒ‡ç¤ºã€‘\n"
                f"{override_prompt}"
            )

            outputs = []
            progress_bar = st.progress(0)
            status_text = st.empty()

            for idx, step in enumerate(pipeline["steps"]):
                agent_id = step["agent_id"]
                agent_cfg = next((a for a in config["agents"] if a["id"] == agent_id), None)

                if not agent_cfg:
                    st.error(f"âŒ æ‰¾ä¸åˆ°ä»£ç†è¨­å®šï¼š{agent_id}")
                    return

                progress = (idx + 1) / len(pipeline["steps"])
                progress_bar.progress(progress)
                status_text.text(f"åŸ·è¡Œä»£ç†ï¼š{agent_cfg['name']} ...")

                try:
                    result = run_agent(
                        agent_cfg=agent_cfg,
                        user_prompt=current_input,
                        override_provider=None if provider.startswith("(") else provider,
                        override_model=model_override or None,
                        max_tokens=st.session_state.get("default_max_tokens", 1024),
                        temperature=st.session_state.get("default_temperature", 0.7),
                    )
                    outputs.append({"agent_id": agent_id, "output": result})
                    current_input = result
                    update_player_stats("regenerate")
                except Exception as e:
                    st.error(f"âŒ æ¨¡å‹å‘¼å«å¤±æ•—ï¼š{e}")
                    add_combat_log(f"å¯©æŸ¥æµç¨‹åœ¨ä»£ç† {agent_id} ä¸­æ–·ã€‚", "error")
                    return

            progress_bar.progress(1.0)
            status_text.text("âœ… å¯©æŸ¥æµç¨‹å®Œæˆã€‚")

            st.success("ğŸ‰ å¯©æŸ¥æµç¨‹å·²æˆåŠŸå®Œæˆä¸¦ç”¢å‡ºçµæœã€‚")
            update_player_stats("quest_complete")
            add_combat_log(f"å·²å®Œæˆå¯©æŸ¥æµç¨‹ï¼š{selected_name}", "success")

            st.session_state.pipeline_history.append(outputs)

            st.markdown("### ğŸ“˜ æµç¨‹è¼¸å‡ºçµæœ")
            for idx, item in enumerate(outputs, start=1):
                with st.expander(f"æ­¥é©Ÿ {idx} â€“ ä»£ç† `{item['agent_id']}`", expanded=(idx == len(outputs))):
                    st.markdown(item["output"])

    with col2:
        render_activity_log()
        st.markdown("### ğŸ“Š æµç¨‹çµ±è¨ˆ")
        st.metric("å·²åŸ·è¡Œæµç¨‹æ¬¡æ•¸", len(st.session_state.pipeline_history))

# -----------------------------------------------------------
# Smart Replace Tab (placeholder, original feature kept)
# -----------------------------------------------------------

def render_smart_replace_tab():
    """Placeholder for smart editing (original feature kept)"""
    st.markdown(f"## âœ¨ {get_translation('smart_replace')}")
    st.info("æ­¤å€å¯æ•´åˆæ—¢æœ‰æ–‡å­—æ”¹å¯«èˆ‡æ¯”å°å·¥å…·ï¼ˆä¿ç•™åŸå§‹è¨­è¨ˆç©ºé–“ï¼‰ã€‚")

# -----------------------------------------------------------
# AI Note Keeper: helpers
# -----------------------------------------------------------

def highlight_keywords_in_text(text: str, keywords: List[str], color: str) -> str:
    """Highlight given keywords in text using HTML span with specified color"""
    if not text or not keywords:
        return text
    result = text
    for kw in keywords:
        kw = kw.strip()
        if not kw:
            continue
        pattern = re.compile(re.escape(kw), re.IGNORECASE)
        result = pattern.sub(
            lambda m: f"<span style='color:{color}'>{m.group(0)}</span>",
            result,
        )
    return result

# -----------------------------------------------------------
# AI Note Keeper Tab
# -----------------------------------------------------------

def render_notes_tab():
    """Render AI Note Keeper with multiple AI tools"""
    st.markdown(f"## ğŸ“” {get_translation('notes')}")
    st.info(
        "å°‡ 510(k) æˆ–é†«ç™‚å™¨æç›¸é—œæ–‡å­—è²¼ä¸Šï¼Œåˆ©ç”¨å¤šä»£ç† AI é€²è¡Œ **Markdown çµæ§‹åŒ–ã€æ ¼å¼å„ªåŒ–ã€é—œéµå­—æ¨™ç¤ºã€å¯¦é«”æŠ½å–ã€å¿ƒæ™ºåœ–èˆ‡è©å½™é—œè¯åœ–**ã€‚"
    )

    col1, col2 = st.columns([2, 1])
    with col1:
        st.text_area(
            "ğŸ§¾ åŸå§‹æ–‡æœ¬è²¼ä¸Šå€",
            key="note_raw_text",
            height=260,
            help="ä¾‹å¦‚ï¼š510(k) æ‘˜è¦ã€é¢¨éšªç®¡ç†å ±å‘Šç‰‡æ®µã€æŠ€è¡“èªªæ˜ã€å›è¦† FDA å•ç­”ç­‰",
        )
        if st.button("ğŸ“„ è½‰æ›ç‚º Markdown çµæ§‹", use_container_width=True):
            if not st.session_state.note_raw_text.strip():
                st.warning("è«‹å…ˆè²¼ä¸ŠåŸå§‹æ–‡æœ¬ã€‚")
            else:
                try:
                    provider = st.session_state.get("default_provider", "openai")
                    model = st.session_state.get("default_model", "gpt-4o-mini")
                    system_prompt = (
                        "You are a professional FDA 510(k) regulatory note architect.\n"
                        "Goal: Convert the raw text into a **lossless, well-structured Markdown document**.\n"
                        "- Preserve all original factual content (no deletions, no hallucinations).\n"
                        "- You MAY:\n"
                        "  - Split or merge paragraphs for readability.\n"
                        "  - Introduce hierarchical headings (##, ###) that reflect regulatory logic (device, indications, SE, testing, risk, clinical, labeling, etc.).\n"
                        "  - Use bullet / numbered lists where appropriate.\n"
                        "- You MUST NOT:\n"
                        "  - Omit meaningful information.\n"
                        "  - Add new data not present in the source.\n"
                        "Output: Markdown only. Do not add any explanation outside the Markdown content."
                    )
                    user_prompt = st.session_state.note_raw_text
                    md = call_llm(
                        provider=provider,
                        model=model,
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        max_tokens=st.session_state.get("default_max_tokens", 1024),
                        temperature=0.1,
                    )
                    st.session_state.note_markdown = md
                    add_combat_log("å®ŒæˆåŸå§‹æ–‡æœ¬çš„ Markdown çµæ§‹åŒ–ã€‚", "success")
                except Exception as e:
                    st.error(f"è½‰æ›ç‚º Markdown æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    with col2:
        st.markdown("### ğŸ“‘ Markdown é è¦½")
        if st.session_state.note_markdown:
            st.markdown(st.session_state.note_markdown)
        else:
            st.caption("å°šæœªç”¢ç”Ÿ Markdownï¼Œè«‹å…ˆæ–¼å·¦å´è²¼ä¸Šæ–‡å­—ä¸¦æŒ‰ä¸‹ã€Œè½‰æ›ç‚º Markdownã€ã€‚")

    st.markdown("---")

    tab_fmt, tab_kw, tab_ent, tab_mind, tab_word = st.tabs(
        ["AI æ ¼å¼å„ªåŒ–", "AI é—œéµå­—æ¨™ç¤º", "AI å¯¦é«”æŠ½å–", "AI å¿ƒæ™ºåœ–", "AI è©å½™é—œè¯åœ–"]
    )

    # --- AI Formatting ---
    with tab_fmt:
        st.markdown("### ğŸ§¹ AI æ ¼å¼å„ªåŒ–ï¼ˆä¿ç•™åŸæ–‡ï¼Œå¼·åŒ–çµæ§‹èˆ‡é‡é»ï¼‰")
        st.caption(
            "èªªæ˜ï¼šåœ¨**ä¸åˆªé™¤ä»»ä½•åŸæ–‡å¥å­**çš„å‰æä¸‹ï¼Œé‡æ–°ç·¨æ’æ®µè½èˆ‡æ¨™é¡Œï¼Œä¸¦ç”¨çŠç‘šè‰²æ¨™è¨»é‡è¦è¡“èªã€‚"
        )
        if st.button("âš™ï¸ åŸ·è¡Œ AI æ ¼å¼å„ªåŒ–", use_container_width=True, key="btn_ai_format"):
            base_text = st.session_state.note_markdown or st.session_state.note_raw_text
            if not base_text.strip():
                st.warning("è«‹å…ˆè²¼ä¸Šæ–‡å­—ä¸¦è‡³å°‘å®Œæˆä¸€æ¬¡ Markdown è½‰æ›ã€‚")
            else:
                try:
                    provider = st.session_state.get("default_provider", "openai")
                    model = st.session_state.get("default_model", "gpt-4o-mini")
                    system_prompt = (
                        "You are an expert editor for FDA 510(k) submissions.\n"
                        "Task: Reformat the provided content while **preserving every original sentence**.\n"
                        "You MUST:\n"
                        "- Keep all sentences and technical terms intact (no deletion, no paraphrasing).\n"
                        "- Re-group paragraphs logically (device description, indications, SE, testing, risk, clinical, labeling, etc.).\n"
                        "- Add meaningful Markdown headings (##, ###) and lists.\n"
                        "- Wrap HIGH-VALUE regulatory/technical/clinical keywords with HTML spans:\n"
                        "  <span style=\"color:coral\">keyword</span>\n"
                        "Output: Markdown + inline HTML only. No extra commentary."
                    )
                    user_prompt = base_text
                    formatted = call_llm(
                        provider=provider,
                        model=model,
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        max_tokens=st.session_state.get("default_max_tokens", 2048),
                        temperature=0.4,
                    )
                    st.session_state.note_formatted = formatted
                    add_combat_log("å®Œæˆ AI æ ¼å¼å„ªåŒ–èˆ‡é‡é»æ¨™ç¤ºã€‚", "success")
                except Exception as e:
                    st.error(f"AI æ ¼å¼å„ªåŒ–å¤±æ•—ï¼š{e}")

        if st.session_state.note_formatted:
            st.markdown("#### æ ¼å¼å„ªåŒ–çµæœ")
            st.markdown(st.session_state.note_formatted, unsafe_allow_html=True)

    # --- AI Keywords ---
    with tab_kw:
        st.markdown("### ğŸ¯ AI é—œéµå­—æ¨™ç¤º")
        st.caption("å¯è‡ªè¨‚æ¬²å¼·èª¿çš„é—œéµè©èˆ‡é¡è‰²ï¼Œåœ¨ Markdown å…§å®¹ä¸­è‡ªå‹•é«˜äº®ã€‚")

        kw_text = st.text_input(
            "è¼¸å…¥æ¬²æ¨™ç¤ºçš„é—œéµå­—ï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰",
            value="510(k), substantial equivalence, risk management, performance testing, FDA",
        )
        kw_color = st.color_picker("é—œéµå­—é¡è‰²", value="#FF7F50")

        if st.button("ğŸ” æ¨™ç¤ºé—œéµå­—", use_container_width=True):
            base_text = (
                st.session_state.note_formatted
                or st.session_state.note_markdown
                or st.session_state.note_raw_text
            )
            if not base_text.strip():
                st.warning("å°šç„¡å¯è™•ç†çš„æ–‡æœ¬ï¼Œè«‹å…ˆç”¢ç”Ÿ Markdown æˆ–è²¼ä¸Šæ–‡å­—ã€‚")
            else:
                keywords = [k for k in kw_text.split(",") if k.strip()]
                highlighted = highlight_keywords_in_text(base_text, keywords, kw_color)
                st.session_state.note_keywords_output = highlighted
                add_combat_log("å®Œæˆè‡ªè¨‚é—œéµå­—æ¨™ç¤ºã€‚", "success")

        if st.session_state.note_keywords_output:
            st.markdown("#### é—œéµå­—æ¨™ç¤ºçµæœ")
            st.markdown(st.session_state.note_keywords_output, unsafe_allow_html=True)

    # --- AI Entities ---
    with tab_ent:
        st.markdown("### ğŸ§¬ AI å¯¦é«”æŠ½å–ï¼ˆæœ€å¤š 20 å€‹ï¼‰")
        st.caption(
            "å¾æ–‡æœ¬ä¸­æŠ½å–æœ€é‡è¦çš„æ³•è¦ã€æŠ€è¡“ã€è‡¨åºŠèˆ‡é¢¨éšªç›¸é—œå¯¦é«”ï¼Œä¸¦ç”¢ç”Ÿçµæ§‹åŒ–è¡¨æ ¼èˆ‡ JSONã€‚"
        )
        if st.button("ğŸ“Š æŠ½å– 20 å€‹é—œéµå¯¦é«”", use_container_width=True):
            base_text = st.session_state.note_markdown or st.session_state.note_raw_text
            if not base_text.strip():
                st.warning("è«‹å…ˆè²¼ä¸Šæ–‡å­—ä¸¦è‡³å°‘å®Œæˆä¸€æ¬¡ Markdown è½‰æ›ã€‚")
            else:
                try:
                    provider = st.session_state.get("default_provider", "openai")
                    model = st.session_state.get("default_model", "gpt-4o-mini")
                    system_prompt = (
                        "You are an information extraction specialist for FDA 510(k) dossiers.\n"
                        "From the provided text, identify up to 20 highest-value entities. Entities may be:\n"
                        "- regulations or standards\n"
                        "- submission sections (e.g., Indications for Use, Device Description)\n"
                        "- device modules or components\n"
                        "- risk types or hazards\n"
                        "- performance tests\n"
                        "- clinical endpoints or outcomes\n"
                        "Return **JSON only** in the form:\n"
                        "[\n"
                        "  {\"id\": 1, \"name\": \"...\", \"type\": \"regulation|section|risk|test|clinical|other\", "
                        "\"description\": \"short explanation\", \"source_snippet\": \"representative phrase from text\"},\n"
                        "  ... up to 20 entities\n"
                        "]"
                    )
                    user_prompt = base_text
                    raw = call_llm(
                        provider=provider,
                        model=model,
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        max_tokens=1024,
                        temperature=0.2,
                    )
                    raw_str = raw.strip().strip("```json").strip("```").strip()
                    entities = json.loads(raw_str)
                    if not isinstance(entities, list):
                        raise ValueError("å›å‚³å…§å®¹ä¸¦é JSON é™£åˆ—ã€‚")
                    st.session_state.note_entities_json_data = entities
                    add_combat_log("å®Œæˆæ–‡æœ¬å¯¦é«”æŠ½å–ï¼ˆæœ€å¤š 20 å€‹ï¼‰ã€‚", "success")
                except Exception as e:
                    st.error(f"å¯¦é«”æŠ½å–èˆ‡ JSON è§£æå¤±æ•—ï¼š{e}")

        if st.session_state.note_entities_json_data:
            st.markdown("#### å¯¦é«”è¡¨æ ¼")
            table_md = "| id | name | type | description | source_snippet |\n"
            table_md += "|---|------|------|-------------|----------------|\n"
            for ent in st.session_state.note_entities_json_data:
                table_md += (
                    f"| {ent.get('id','')} "
                    f"| {ent.get('name','')} "
                    f"| {ent.get('type','')} "
                    f"| {ent.get('description','').replace('|','/')} "
                    f"| {ent.get('source_snippet','').replace('|','/')} |\n"
                )
            st.markdown(table_md)

            st.markdown("#### JSON æª¢è¦–")
            st.json(st.session_state.note_entities_json_data)

    # --- AI Mind-Map ---
    with tab_mind:
        st.markdown("### ğŸ§  AI å¿ƒæ™ºåœ–")
        st.caption(
            "æ ¹æ“šæ–‡æœ¬å…§å®¹è‡ªå‹•ç”¢ç”Ÿç¯€é»èˆ‡é—œä¿‚çš„ JSONï¼Œæ‚¨å¯æ‰‹å‹•èª¿æ•´å¾Œï¼Œå³æ™‚è¦–è¦ºåŒ–ç‚ºå¿ƒæ™ºåœ–ã€‚"
        )
        if st.button("ğŸ§  ç”¢ç”Ÿå¿ƒæ™ºåœ– JSON", use_container_width=True):
            base_text = st.session_state.note_markdown or st.session_state.note_raw_text
            if not base_text.strip():
                st.warning("è«‹å…ˆè²¼ä¸Šæ–‡å­—ä¸¦è‡³å°‘å®Œæˆä¸€æ¬¡ Markdown è½‰æ›ã€‚")
            else:
                try:
                    provider = st.session_state.get("default_provider", "openai")
                    model = st.session_state.get("default_model", "gpt-4o-mini")
                    system_prompt = (
                        "You are a knowledge graph designer.\n"
                        "Create a compact **mind-map JSON** from the text:\n"
                        "{\n"
                        "  \"nodes\": [\n"
                        "    {\"id\": \"NodeID\", \"label\": \"display name\", \"type\": \"device|risk|test|regulation|clinical|other\"},\n"
                        "    ... 8â€“15 nodes\n"
                        "  ],\n"
                        "  \"edges\": [\n"
                        "    {\"source\": \"NodeID\", \"target\": \"NodeID\", \"relation\": \"short description\"},\n"
                        "    ... 10â€“25 edges\n"
                        "  ]\n"
                        "}\n"
                        "Output JSON only."
                    )
                    user_prompt = base_text
                    raw = call_llm(
                        provider=provider,
                        model=model,
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        max_tokens=1024,
                        temperature=0.3,
                    )
                    raw_str = raw.strip().strip("```json").strip("```").strip()
                    st.session_state.note_mindmap_json_text = raw_str
                    add_combat_log("å·²ç”¢ç”Ÿå¿ƒæ™ºåœ– JSON çµæ§‹ã€‚", "success")
                except Exception as e:
                    st.error(f"å¿ƒæ™ºåœ– JSON ç”¢ç”Ÿå¤±æ•—ï¼š{e}")

        mindmap_text = st.text_area(
            "å¿ƒæ™ºåœ– JSON å¯æ–¼æ­¤èª¿æ•´å¾Œé‡æ–°ç¹ªè£½",
            value=st.session_state.note_mindmap_json_text,
            height=220,
        )
        if st.button("ğŸ“ˆ æ ¹æ“š JSON é¡¯ç¤ºå¿ƒæ™ºåœ–", use_container_width=True):
            try:
                data = json.loads(mindmap_text)
                nodes = data.get("nodes", [])
                edges = data.get("edges", [])
                dot = "digraph G {\nrankdir=LR;\n"
                for n in nodes:
                    nid = n.get("id", "")
                    label = n.get("label", nid)
                    dot += f"  \"{nid}\" [label=\"{label}\"];\n"
                for e in edges:
                    src = e.get("source", "")
                    tgt = e.get("target", "")
                    rel = e.get("relation", "")
                    dot += f"  \"{src}\" -> \"{tgt}\" [label=\"{rel}\"];\n"
                dot += "}"
                st.graphviz_chart(dot)
            except Exception as e:
                st.error(f"è§£ææˆ–ç¹ªè£½å¿ƒæ™ºåœ–æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    # --- AI Wordgraph ---
    with tab_word:
        st.markdown("### ğŸ“š AI è©å½™é—œè¯åœ– (Wordgraph)")
        st.caption(
            "æ ¹æ“šæ–‡æœ¬è‡ªå‹•åˆ†æé‡è¦è¡“èªä¹‹é–“çš„é—œè¯ï¼Œç”¢ç”Ÿè©å½™é—œè¯åœ– JSON ä¸¦è¦–è¦ºåŒ–ã€‚"
        )
        if st.button("ğŸ“š ç”¢ç”Ÿè©å½™é—œè¯ JSON", use_container_width=True):
            base_text = st.session_state.note_markdown or st.session_state.note_raw_text
            if not base_text.strip():
                st.warning("è«‹å…ˆè²¼ä¸Šæ–‡å­—ä¸¦è‡³å°‘å®Œæˆä¸€æ¬¡ Markdown è½‰æ›ã€‚")
            else:
                try:
                    provider = st.session_state.get("default_provider", "openai")
                    model = st.session_state.get("default_model", "gpt-4o-mini")
                    system_prompt = (
                        "You are a text mining and terminology network expert.\n"
                        "From the text, select 10â€“15 key technical/regulatory/clinical terms and "
                        "build a wordgraph JSON:\n"
                        "{\n"
                        "  \"nodes\": [\n"
                        "    {\"id\": \"TermID\", \"label\": \"display name\", \"frequency\": number},\n"
                        "    ...\n"
                        "  ],\n"
                        "  \"edges\": [\n"
                        "    {\"source\": \"TermID\", \"target\": \"TermID\", \"weight\": 1-5, \"note\": \"link explanation\"},\n"
                        "    ...\n"
                        "  ]\n"
                        "}\n"
                        "Output JSON only."
                    )
                    user_prompt = base_text
                    raw = call_llm(
                        provider=provider,
                        model=model,
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        max_tokens=1024,
                        temperature=0.4,
                    )
                    raw_str = raw.strip().strip("```json").strip("```").strip()
                    st.session_state.note_wordgraph_json_text = raw_str
                    add_combat_log("å·²ç”¢ç”Ÿè©å½™é—œè¯åœ– JSON çµæ§‹ã€‚", "success")
                except Exception as e:
                    st.error(f"è©å½™é—œè¯ JSON ç”¢ç”Ÿå¤±æ•—ï¼š{e}")

        wordgraph_text = st.text_area(
            "è©å½™é—œè¯åœ– JSON å¯æ–¼æ­¤èª¿æ•´å¾Œé‡æ–°ç¹ªè£½",
            value=st.session_state.note_wordgraph_json_text,
            height=220,
        )
        if st.button("ğŸ“Š æ ¹æ“š JSON é¡¯ç¤ºè©å½™é—œè¯åœ–", use_container_width=True):
            try:
                data = json.loads(wordgraph_text)
                nodes = data.get("nodes", [])
                edges = data.get("edges", [])
                dot = "graph G {\n"
                for n in nodes:
                    nid = n.get("id", "")
                    label = n.get("label", nid)
                    freq = n.get("frequency", 1)
                    size = 10 + freq * 2
                    dot += f"  \"{nid}\" [label=\"{label}\", fontsize={size}];\n"
                for e in edges:
                    src = e.get("source", "")
                    tgt = e.get("target", "")
                    w = e.get("weight", 1)
                    note = e.get("note", "")
                    penwidth = 1 + w
                    dot += (
                        f"  \"{src}\" -- \"{tgt}\" "
                        f"[label=\"{note}\", penwidth={penwidth}];\n"
                    )
                dot += "}"
                st.graphviz_chart(dot)
            except Exception as e:
                st.error(f"è§£ææˆ–ç¹ªè£½è©å½™é—œè¯åœ–æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

# -----------------------------------------------------------
# Submission OCR Studio â€“ helpers
# -----------------------------------------------------------

def parse_page_selection(pages_str: str, max_pages: int) -> List[int]:
    """
    Parse a page selection string like "1-3,5" into a sorted list of 1-based page numbers.
    """
    if not pages_str:
        return list(range(1, max_pages + 1))
    pages_str = pages_str.replace(" ", "")
    pages: List[int] = []
    for part in pages_str.split(","):
        if "-" in part:
            start_s, end_s = part.split("-", 1)
            if not start_s.isdigit() or not end_s.isdigit():
                continue
            start, end = int(start_s), int(end_s)
            if start <= 0 or end <= 0:
                continue
            for p in range(start, end + 1):
                if 1 <= p <= max_pages:
                    pages.append(p)
        else:
            if part.isdigit():
                p = int(part)
                if 1 <= p <= max_pages:
                    pages.append(p)
    pages = sorted(set(pages))
    if not pages:
        pages = [1]
    return pages

def ensure_pdf_reader():
    if PdfReader is None:
        raise RuntimeError("PyPDF2 æœªå®‰è£ï¼Œç„¡æ³•è®€å– PDFã€‚è«‹åœ¨ç’°å¢ƒä¸­å®‰è£ PyPDF2ã€‚")

def ensure_tesseract():
    if pytesseract is None or convert_from_bytes is None:
        raise RuntimeError("pytesseract æˆ– pdf2image æœªå®‰è£ï¼Œç„¡æ³•åŸ·è¡Œ Python OCRã€‚")

def get_pdf_page_count(pdf_bytes: bytes) -> int:
    ensure_pdf_reader()
    reader = PdfReader(BytesIO(pdf_bytes))
    return len(reader.pages)

def extract_pdf_text(pdf_bytes: bytes, pages: List[int]) -> str:
    """Extract textual content from specified 1-based pages using PyPDF2"""
    ensure_pdf_reader()
    reader = PdfReader(BytesIO(pdf_bytes))
    texts: List[str] = []
    for p in pages:
        if 1 <= p <= len(reader.pages):
            page = reader.pages[p - 1]
            txt = page.extract_text() or ""
            texts.append(f"\n\n--- Page {p} ---\n\n{txt}")
    return "\n".join(texts).strip()

def ocr_pdf_tesseract(pdf_bytes: bytes, pages: List[int], lang: str) -> str:
    """OCR selected pages using Tesseract (english / traditional chinese)"""
    ensure_tesseract()
    first_page, last_page = min(pages), max(pages)
    images = convert_from_bytes(pdf_bytes, first_page=first_page, last_page=last_page)
    result_chunks: List[str] = []
    for idx, img in enumerate(images, start=first_page):
        if idx in pages:
            text = pytesseract.image_to_string(img, lang=lang)
            result_chunks.append(f"\n\n--- Page {idx} ---\n\n{text}")
    return "\n".join(result_chunks).strip()

def pdf_to_base64_iframe(pdf_bytes: bytes, width: str = "100%", height: str = "600") -> str:
    """Generate an HTML iframe to preview a PDF from bytes."""
    b64 = base64.b64encode(pdf_bytes).decode("utf-8")
    return f"""
    <iframe
        src="data:application/pdf;base64,{b64}#toolbar=0"
        width="{width}"
        height="{height}"
        style="border-radius: 12px; border: 1px solid rgba(148,163,184,0.5);"
    ></iframe>
    """

# -----------------------------------------------------------
# Submission OCR Studio Tab
# -----------------------------------------------------------

ADVANCED_OCR_SYSTEM_PROMPT = """
You are an elite OCR + document reconstruction assistant specialized in FDA 510(k) submissions.
Input: noisy text extracted from PDF pages (including possible encoding issues, line breaks, hyphenations).
Your tasks:
1. Denoise and normalize:
   - Fix broken words and hyphenation at line breaks.
   - Remove obvious OCR noise (random symbols, page headers/footers if clearly repetitive).
   - Preserve all substantive regulatory, clinical, risk and technical information.
2. Reconstruct structure as Markdown:
   - Introduce clear headings (##, ###) for sections like: Device Description, Indications for Use, Substantial Equivalence, Performance Testing, Risk Management, Clinical, Labeling, etc., when they are present or inferable.
   - Use bullet/numbered lists to improve readability.
3. Coral keyword highlighting:
   - Wrap high-value domain keywords with: <span style="color:coral">keyword</span>.
   - Focus on: device name, key parameters, standards, risk terms, clinical endpoints, important performance metrics, critical regulatory references.
Constraints:
- Do NOT invent new facts.
- Do NOT omit meaningful content.
Output:
- Return **Markdown + inline HTML only**, ready to render in a viewer.
"""

def render_submission_ocr_tab():
    """Render multi-file Submission OCR Studio with PDF/TXT upload + OCR + summaries + combined QA"""
    st.markdown(f"## ğŸ“‚ {get_translation('ocr')}")

    st.info(
        "æ­¤åˆ†é å¯è™•ç†å¤šå€‹ PDF / TXT é€ä»¶è³‡æ–™ï¼š\n"
        "1ï¸âƒ£ é¸æ“‡æ¬²ä¸Šå‚³æª”æ¡ˆæ•¸é‡ â†’ 2ï¸âƒ£ ä¸Šå‚³ PDF/TXT â†’ 3ï¸âƒ£ ç‚ºæ¯ä»½æª”æ¡ˆé¸æ“‡é ç¢¼èˆ‡ OCR æ–¹å¼\n"
        "4ï¸âƒ£ ç‚ºæ¯ä»½æª”ç”¢ç”Ÿ **Markdownï¼ˆå«çŠç‘šè‰²é—œéµå­—ï¼‰** èˆ‡æ‘˜è¦ â†’ 5ï¸âƒ£ å°æ‰€æœ‰ OCR æ–‡ä»¶æ•´åˆæŠ½å– 20 å€‹å¯¦é«”ä¸¦é€²è¡Œæå•ã€‚"
    )

    # Step 0 â€“ global keyword highlight config
    st.markdown("### ğŸ¯ å…¨åŸŸé—œéµå­—è¨­å®šï¼ˆé©ç”¨æ–¼ Python OCR ç”¢ç‰©ï¼‰")
    st.text_input(
        "åœ¨ OCR çµæœä¸­æ¬²ä»¥çŠç‘šè‰²æ¨™ç¤ºçš„é—œéµå­—ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œå¯ä¸­è‹±æ··åˆï¼‰",
        key="ocr_global_keywords",
    )

    # Step 1 â€“ user-estimated number of files
    num_files = st.number_input("é è¨ˆè™•ç†çš„æª”æ¡ˆæ•¸é‡", min_value=1, max_value=20, value=1, step=1)

    # Step 2 â€“ upload files
    uploaded_files = st.file_uploader(
        "ä¸Šå‚³ PDF / TXT æª”æ¡ˆï¼ˆå¯å¤šé¸ï¼‰",
        type=["pdf", "txt"],
        accept_multiple_files=True,
    )

    if uploaded_files:
        if len(uploaded_files) != num_files:
            st.warning(f"ç›®å‰å·²ä¸Šå‚³ {len(uploaded_files)} å€‹æª”æ¡ˆï¼Œèˆ‡é è¨ˆæ•¸é‡ {num_files} ä¸åŒï¼Œå¯è¦–éœ€è¦èª¿æ•´ã€‚")

        # Rebuild or update state for ocr_files
        existing_by_name = {f["filename"]: f for f in st.session_state.ocr_files}
        new_state_files: List[Dict[str, Any]] = []

        for uf in uploaded_files:
            name = uf.name
            ext = "pdf" if name.lower().endswith(".pdf") else "txt"
            content = uf.getvalue()

            prev = existing_by_name.get(name, {})
            entry = {
                "filename": name,
                "ext": ext,
                "bytes": content,
                "num_pages": prev.get("num_pages"),
                "markdown": prev.get("markdown", ""),
                "summary": prev.get("summary", ""),
            }
            if ext == "pdf" and entry["num_pages"] is None:
                try:
                    entry["num_pages"] = get_pdf_page_count(content)
                except Exception as e:
                    st.error(f"ç„¡æ³•è®€å– PDF é æ•¸ï¼š{name} - {e}")
                    entry["num_pages"] = 0

            new_state_files.append(entry)

        st.session_state.ocr_files = new_state_files

        st.markdown("### ğŸ“š æª”æ¡ˆè¨­å®šèˆ‡ OCR é¸é …")

        for idx, file_info in enumerate(st.session_state.ocr_files):
            fname = file_info["filename"]
            ext = file_info["ext"]
            key_prefix = f"ocr_{idx}"

            with st.expander(f"{idx+1}. {fname}", expanded=True):
                if ext == "pdf" and file_info.get("bytes"):
                    st.markdown("#### ğŸ“– PDF é è¦½")
                    try:
                        iframe_html = pdf_to_base64_iframe(file_info["bytes"], height="480")
                        st.components.v1.html(iframe_html, height=500, scrolling=True)
                    except Exception:
                        st.info("ç€è¦½å™¨æˆ–ç’°å¢ƒé™åˆ¶ï¼ŒPDF å…§åµŒé è¦½å¤±æ•—ï¼Œå¯æ”¹ç”¨ä¸‹è¼‰æª¢è¦–ã€‚")
                        st.download_button("ä¸‹è¼‰ PDF", data=file_info["bytes"], file_name=fname)

                    num_pages = file_info.get("num_pages", 0)
                    st.markdown(f"- ç¸½é æ•¸ï¼š**{num_pages}**")

                    pages_default = st.session_state.get(f"{key_prefix}_pages_str", "1-3" if num_pages >= 3 else "1")
                    pages_str = st.text_input(
                        "æ¬² OCR çš„é ç¢¼ï¼ˆä¾‹å¦‚ï¼š1-3,5ï¼‰",
                        value=pages_default,
                        key=f"{key_prefix}_pages_str",
                    )

                    ocr_backend = st.radio(
                        "OCR æ–¹å¼",
                        ["Python OCR (Tesseract)", "LLM-based OCR (å¤šæ¨¡å‹æ”¯æ´)"],
                        key=f"{key_prefix}_backend",
                    )

                    if ocr_backend.startswith("Python"):
                        lang_label = st.selectbox(
                            "Tesseract èªè¨€",
                            ["English", "Traditional Chinese", "English + Traditional Chinese"],
                            key=f"{key_prefix}_lang",
                        )
                        if lang_label == "English":
                            lang_code = "eng"
                        elif lang_label == "Traditional Chinese":
                            lang_code = "chi_tra"
                        else:
                            lang_code = "eng+chi_tra"
                    else:
                        lang_code = None  # not used

                        st.markdown("##### LLM OCR è¨­å®š")
                        col_l1, col_l2 = st.columns(2)
                        with col_l1:
                            llm_provider = st.selectbox(
                                "ä¾›æ‡‰å•†",
                                ["openai", "gemini", "xai", "anthropic"],
                                key=f"{key_prefix}_llm_provider",
                            )
                        with col_l2:
                            provider_models = {
                                "openai": ["gpt-5-nano", "gpt-4o-mini", "gpt-4.1-mini"],
                                "gemini": ["gemini-2.5-flash", "gemini-2.5-flash-lite"],
                                "xai": ["grok-4-fast-reasoning", "grok-3-mini"],
                                "anthropic": ["claude-3-5-sonnet-latest", "claude-3-opus-latest"],
                            }
                            llm_model = st.selectbox(
                                "æ¨¡å‹",
                                provider_models[llm_provider],
                                key=f"{key_prefix}_llm_model",
                            )

                        llm_max_tokens = st.number_input(
                            "æœ€å¤§è¼¸å‡º tokensï¼ˆOCR/æ¸…ç†ç”¨ï¼‰",
                            min_value=128, max_value=4096, value=1500, step=64,
                            key=f"{key_prefix}_llm_max_tokens",
                        )

                        llm_temp = st.slider(
                            "æº«åº¦ï¼ˆOCR/æ¸…ç†ï¼‰",
                            0.0, 1.0, 0.2, 0.05,
                            key=f"{key_prefix}_llm_temp",
                        )

                        default_ocr_prompt = ADVANCED_OCR_SYSTEM_PROMPT.strip()
                        llm_system_prompt = st.text_area(
                            "é€²éš OCR ç³»çµ±æç¤ºï¼ˆå¯å¾®èª¿ï¼‰",
                            value=default_ocr_prompt,
                            height=180,
                            key=f"{key_prefix}_llm_system_prompt",
                        )

                    if st.button("â–¶ï¸ åŸ·è¡Œæ­¤æª” OCRï¼ˆè½‰ Markdownï¼‹çŠç‘šè‰²é—œéµå­—ï¼‰", key=f"{key_prefix}_run"):
                        if num_pages <= 0:
                            st.error("ç„¡æ³•å–å¾— PDF é æ•¸ï¼Œè«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦ææ¯€ã€‚")
                        else:
                            try:
                                pages = parse_page_selection(pages_str, num_pages)

                                if ocr_backend.startswith("Python"):
                                    # Python OCR path
                                    if "+" in (lang_code or ""):
                                        langs = lang_code.split("+")
                                        text_agg = ""
                                        for l in langs:
                                            text_agg += ocr_pdf_tesseract(file_info["bytes"], pages, l)
                                        raw_text = text_agg
                                    else:
                                        raw_text = ocr_pdf_tesseract(file_info["bytes"], pages, lang_code)

                                    # Simple Markdown wrap + keyword highlight
                                    markdown_raw = raw_text or ""
                                    kw_str = st.session_state.get("ocr_global_keywords", "")
                                    keywords = [k for k in kw_str.split(",") if k.strip()]
                                    markdown = highlight_keywords_in_text(
                                        markdown_raw, keywords, "#FF7F50"
                                    )
                                    file_info["markdown"] = markdown
                                    st.session_state.ocr_files[idx] = file_info
                                    add_combat_log(f"{fname} å·²å®Œæˆ Python OCRã€‚", "success")

                                else:
                                    # LLM-based OCR / cleanup
                                    text_extracted = extract_pdf_text(file_info["bytes"], pages)
                                    llm_provider = st.session_state.get(f"{key_prefix}_llm_provider", "openai")
                                    llm_model = st.session_state.get(f"{key_prefix}_llm_model", "gpt-4o-mini")
                                    llm_max_tokens = st.session_state.get(f"{key_prefix}_llm_max_tokens", 1500)
                                    llm_temp = st.session_state.get(f"{key_prefix}_llm_temp", 0.2)
                                    llm_system = st.session_state.get(
                                        f"{key_prefix}_llm_system_prompt",
                                        ADVANCED_OCR_SYSTEM_PROMPT.strip(),
                                    )
                                    markdown = call_llm(
                                        provider=llm_provider,
                                        model=llm_model,
                                        system_prompt=llm_system,
                                        user_prompt=text_extracted,
                                        max_tokens=int(llm_max_tokens),
                                        temperature=float(llm_temp),
                                    )
                                    file_info["markdown"] = markdown
                                    st.session_state.ocr_files[idx] = file_info
                                    add_combat_log(f"{fname} å·²å®Œæˆ LLM OCR / æ¸…ç†ã€‚", "success")

                                st.success("âœ… OCR å®Œæˆï¼Œå·²è½‰æ›ç‚º Markdownã€‚")
                                st.markdown("##### OCR Markdown é è¦½")
                                st.markdown(file_info["markdown"], unsafe_allow_html=True)

                            except Exception as e:
                                st.error(f"OCR éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

                else:
                    # TXT file
                    text_content = file_info["bytes"].decode("utf-8", errors="ignore")
                    st.markdown("#### ğŸ“„ TXT å…§å®¹é è¦½ï¼ˆå‰ 800 å­—ï¼‰")
                    st.code(text_content[:800] + ("..." if len(text_content) > 800 else ""))

                    if st.button("â–¶ï¸ å°‡ TXT è½‰ç‚º Markdownï¼ˆå«çŠç‘šè‰²é—œéµå­—ï¼‰", key=f"{key_prefix}_txt_to_md"):
                        try:
                            # Use a light LLM formatting for TXT
                            provider = st.session_state.get("default_provider", "openai")
                            model = st.session_state.get("default_model", "gpt-4o-mini")
                            system_prompt = ADVANCED_OCR_SYSTEM_PROMPT.strip()
                            markdown = call_llm(
                                provider=provider,
                                model=model,
                                system_prompt=system_prompt,
                                user_prompt=text_content,
                                max_tokens=2000,
                                temperature=0.2,
                            )
                            file_info["markdown"] = markdown
                            st.session_state.ocr_files[idx] = file_info
                            add_combat_log(f"{fname} TXT å·²è½‰æ›ç‚ºçµæ§‹åŒ– Markdownã€‚", "success")
                            st.success("âœ… TXT å·²è½‰ç‚º Markdownã€‚")
                            st.markdown("##### Markdown é è¦½")
                            st.markdown(markdown, unsafe_allow_html=True)
                        except Exception as e:
                            st.error(f"TXT è½‰ Markdown å¤±æ•—ï¼š{e}")

                # Per-file summary, if markdown ready
                if file_info.get("markdown"):
                    st.markdown("#### ğŸ§¾ æª”æ¡ˆæ‘˜è¦ï¼ˆå¯è‡ªè¨‚æç¤ºèˆ‡æ¨¡å‹ï¼‰")
                    sum_provider = st.selectbox(
                        "æ‘˜è¦ç”¨æ¨¡å‹ä¾›æ‡‰å•†",
                        ["openai", "gemini", "xai", "anthropic"],
                        key=f"{key_prefix}_sum_provider",
                    )
                    provider_models = {
                        "openai": ["gpt-5-nano", "gpt-4o-mini", "gpt-4.1-mini"],
                        "gemini": ["gemini-2.5-flash", "gemini-2.5-flash-lite"],
                        "xai": ["grok-4-fast-reasoning", "grok-3-mini"],
                        "anthropic": ["claude-3-5-sonnet-latest", "claude-3-opus-latest"],
                    }
                    sum_model = st.selectbox(
                        "æ‘˜è¦æ¨¡å‹",
                        provider_models[sum_provider],
                        key=f"{key_prefix}_sum_model",
                    )
                    sum_tokens = st.number_input(
                        "æœ€å¤§æ‘˜è¦ tokens",
                        min_value=128, max_value=4096, value=800, step=64,
                        key=f"{key_prefix}_sum_tokens",
                    )

                    default_sum_prompt = (
                        "You are a senior FDA 510(k) reviewer.\n"
                        "Summarize this single document into a **concise yet comprehensive regulatory briefing**.\n"
                        "Include:\n"
                        "- Device overview and intended use\n"
                        "- Indications for Use (if present)\n"
                        "- Key technological characteristics\n"
                        "- Substantial equivalence argument highlight\n"
                        "- Major performance tests (bench, biocompatibility, EMC, software, etc.)\n"
                        "- Main risks and mitigations\n"
                        "- Any clinical data or rationale\n"
                        "Return Markdown with clear headings and bullet lists. Do not hallucinate."
                    )
                    custom_sum_prompt = st.text_area(
                        "é€²éšæ‘˜è¦ç³»çµ±æç¤ºï¼ˆå¯èª¿æ•´ï¼‰",
                        value=default_sum_prompt,
                        height=160,
                        key=f"{key_prefix}_sum_prompt",
                    )

                    if st.button("ğŸ§¾ ç”¢ç”Ÿæ­¤æª”çš„å°ˆæ¥­æ‘˜è¦", key=f"{key_prefix}_run_summary"):
                        try:
                            summary = call_llm(
                                provider=sum_provider,
                                model=sum_model,
                                system_prompt=custom_sum_prompt,
                                user_prompt=file_info["markdown"],
                                max_tokens=int(sum_tokens),
                                temperature=0.3,
                            )
                            file_info["summary"] = summary
                            st.session_state.ocr_files[idx] = file_info
                            add_combat_log(f"{fname} å·²ç”¢ç”Ÿæ‘˜è¦ã€‚", "success")
                            st.success("âœ… å·²ç”¢ç”Ÿæ‘˜è¦ã€‚")
                            st.markdown("##### æ‘˜è¦é è¦½")
                            st.markdown(summary, unsafe_allow_html=True)
                        except Exception as e:
                            st.error(f"ç”¢ç”Ÿæ‘˜è¦å¤±æ•—ï¼š{e}")

                    if file_info.get("summary"):
                        with st.expander("ğŸ” ç›®å‰å„²å­˜çš„æ‘˜è¦", expanded=False):
                            st.markdown(file_info["summary"], unsafe_allow_html=True)

    # Combined analysis for all OCR documents
    st.markdown("---")
    st.markdown("### ğŸ”— æ•´åˆæ‰€æœ‰ OCR æ–‡ä»¶ä¸¦åŸ·è¡Œè·¨æ–‡ä»¶åˆ†æ")

    all_markdowns = [
        f"## File {i+1}: {f['filename']}\n\n{f.get('markdown','')}"
        for i, f in enumerate(st.session_state.ocr_files)
        if f.get("markdown")
    ]
    if all_markdowns:
        combined_markdown = "\n\n---\n\n".join(all_markdowns)
        st.session_state.combined_markdown = combined_markdown

        with st.expander("ğŸ“š åˆä½µå¾Œ Markdown é è¦½", expanded=False):
            st.markdown(combined_markdown, unsafe_allow_html=True)

        # Entity extraction across all files
        if st.button("ğŸ§¬ å¾æ‰€æœ‰æ–‡ä»¶ä¸­æŠ½å– 20 å€‹è·¨æ–‡ä»¶é—œéµå¯¦é«”", key="combined_entities_run"):
            try:
                provider = st.session_state.get("default_provider", "openai")
                model = st.session_state.get("default_model", "gpt-4o-mini")
                system_prompt = (
                    "You are a cross-document knowledge extraction specialist for FDA 510(k) dossiers.\n"
                    "You will receive multiple OCR'd documents merged into one Markdown corpus.\n"
                    "Task: Identify up to 20 **cross-document entities** that are most important, such as:\n"
                    "- Device or component names\n"
                    "- Key clinical endpoints / indications\n"
                    "- Critical risks / hazards\n"
                    "- Pivotal performance tests or validation activities\n"
                    "- Referenced standards / guidance documents\n"
                    "For each entity, construct:\n"
                    "{\n"
                    "  \"id\": number,\n"
                    "  \"name\": string,\n"
                    "  \"type\": \"device|risk|test|clinical|regulation|other\",\n"
                    "  \"description\": \"short explanation in 1-3 sentences\",\n"
                    "  \"source_files\": [\"filename1.pdf\", \"filename2.pdf\", ...],\n"
                    "  \"context_snippet\": \"representative excerpt from one or more files\"\n"
                    "}\n"
                    "Output: JSON array only, with at most 20 entities."
                )
                user_prompt = combined_markdown
                raw = call_llm(
                    provider=provider,
                    model=model,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    max_tokens=2000,
                    temperature=0.2,
                )
                raw_str = raw.strip().strip("```json").strip("```").strip()
                entities = json.loads(raw_str)
                if not isinstance(entities, list):
                    raise ValueError("å›å‚³å…§å®¹ä¸¦é JSON é™£åˆ—ã€‚")
                st.session_state.combined_entities = entities
                add_combat_log("å®Œæˆè·¨æ–‡ä»¶ 20 å€‹é—œéµå¯¦é«”æŠ½å–ã€‚", "success")
            except Exception as e:
                st.error(f"è·¨æ–‡ä»¶å¯¦é«”æŠ½å–å¤±æ•—ï¼š{e}")

        if st.session_state.combined_entities:
            st.markdown("#### ğŸ§¬ è·¨æ–‡ä»¶é—œéµå¯¦é«”è¡¨æ ¼")
            table_md = "| id | name | type | description | source_files | context_snippet |\n"
            table_md += "|---|------|------|-------------|--------------|-----------------|\n"
            for ent in st.session_state.combined_entities:
                table_md += (
                    f"| {ent.get('id','')} "
                    f"| {ent.get('name','')} "
                    f"| {ent.get('type','')} "
                    f"| {ent.get('description','').replace('|','/')} "
                    f"| {', '.join(ent.get('source_files', []))} "
                    f"| {ent.get('context_snippet','').replace('|','/')} |\n"
                )
            st.markdown(table_md)

            with st.expander("JSON æª¢è¦–", expanded=False):
                st.json(st.session_state.combined_entities)

        # Prompting on combined document
        st.markdown("### ğŸ’¬ å°åˆä½µå¾Œ OCR æ–‡æª”é€²è¡Œæå•")

        qa_prompt = st.text_area(
            "è«‹è¼¸å…¥å°æ•´é«”æ–‡ä»¶çš„æå•æˆ–åˆ†ææŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼šæ•´é«”é¢¨éšªè¼ªå»“ã€SE è«–è­‰æ˜¯å¦ä¸€è‡´ã€å“ªä»½æª”æ¡ˆé¢¨éšªè¼ƒé«˜ï¼‰",
            height=140,
            key="combined_qa_prompt",
        )

        col_q1, col_q2, col_q3, col_q4 = st.columns(4)
        with col_q1:
            qa_provider = st.selectbox(
                "ä¾›æ‡‰å•†",
                ["openai", "gemini", "xai", "anthropic"],
                key="combined_qa_provider",
            )
        with col_q2:
            provider_models = {
                "openai": ["gpt-5-nano", "gpt-4o-mini", "gpt-4.1-mini"],
                "gemini": ["gemini-2.5-flash", "gemini-2.5-flash-lite"],
                "xai": ["grok-4-fast-reasoning", "grok-3-mini"],
                "anthropic": ["claude-3-5-sonnet-latest", "claude-3-opus-latest"],
            }
            qa_model = st.selectbox(
                "æ¨¡å‹",
                provider_models[qa_provider],
                key="combined_qa_model",
            )
        with col_q3:
            qa_max_tokens = st.number_input(
                "æœ€å¤§å›ç­” tokens",
                min_value=128, max_value=4096, value=1200, step=64,
                key="combined_qa_max_tokens",
            )
        with col_q4:
            qa_temp = st.slider(
                "å›ç­”æº«åº¦",
                0.0, 1.0, 0.3, 0.05,
                key="combined_qa_temp",
            )

        if st.button("ğŸ’¬ é‡å°åˆä½µæ–‡ä»¶åŸ·è¡Œæå•", key="combined_qa_run"):
            if not qa_prompt.strip():
                st.warning("è«‹å…ˆè¼¸å…¥æå•å…§å®¹ã€‚")
            else:
                try:
                    system_prompt = (
                        "You are a senior FDA 510(k) reviewer analyzing multiple OCR'd documents.\n"
                        "You will receive a combined Markdown corpus representing all documents, "
                        "followed by a user question.\n"
                        "You MUST:\n"
                        "- Ground all reasoning strictly in the provided corpus.\n"
                        "- Cross-reference documents when needed (e.g., identify which file supports which point).\n"
                        "- Clearly distinguish hypotheses from explicit evidence.\n"
                        "Output: A structured Markdown answer (with headings and bullet lists) aimed at regulatory reviewers."
                    )
                    user_prompt = (
                        "=== COMBINED OCR DOCUMENTS START ===\n"
                        f"{st.session_state.combined_markdown}\n"
                        "=== COMBINED OCR DOCUMENTS END ===\n\n"
                        f"User question:\n{qa_prompt}"
                    )
                    answer = call_llm(
                        provider=qa_provider,
                        model=qa_model,
                        system_prompt=system_prompt,
                        user_prompt=user_prompt,
                        max_tokens=int(qa_max_tokens),
                        temperature=float(qa_temp),
                    )
                    st.session_state.combined_qa_history.append(
                        {"question": qa_prompt, "answer": answer}
                    )
                    st.success("âœ… å·²æ ¹æ“šåˆä½µæ–‡ä»¶å®Œæˆå›ç­”ã€‚")
                    st.markdown("#### å›ç­”")
                    st.markdown(answer, unsafe_allow_html=True)
                except Exception as e:
                    st.error(f"åˆä½µæ–‡ä»¶æå•å¤±æ•—ï¼š{e}")

        if st.session_state.combined_qa_history:
            with st.expander("ğŸ§¾ æ­·å² Q&A", expanded=False):
                for i, qa in enumerate(reversed(st.session_state.combined_qa_history), start=1):
                    st.markdown(f"**Q{i}:** {qa['question']}")
                    st.markdown(qa["answer"], unsafe_allow_html=True)
                    st.markdown("---")

    else:
        st.info("è«‹å…ˆæ–¼ä¸Šæ–¹ä¸Šå‚³è‡³å°‘ä¸€å€‹ PDF æˆ– TXT æª”æ¡ˆã€‚")

# -----------------------------------------------------------
# Dashboard Tab â€“ enhanced with simple interactive chart
# -----------------------------------------------------------

def render_dashboard_tab():
    """Render interactive dashboard"""
    st.markdown(f"## ğŸ“Š {get_translation('dashboard')}")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("å¯©æŸ¥æˆç†Ÿåº¦ç­‰ç´š", st.session_state.player_level)
    with col2:
        st.metric("å·²å®Œæˆæ¡ˆä»¶æ•¸", st.session_state.quests_completed)
    with col3:
        st.metric("LLM å‘¼å«æ¬¡æ•¸", len(st.session_state.combat_log))
    with col4:
        st.metric("å·²åŸ·è¡Œæµç¨‹æ•¸", len(st.session_state.pipeline_history))

    st.markdown("---")

    dash_tab1, dash_tab2, dash_tab3, dash_tab4 = st.tabs(
        ["æ¡ˆä»¶æ­·ç¨‹", "æ´»å‹•ç´€éŒ„", "é‡Œç¨‹ç¢‘", "äº’å‹•åˆ†æåœ–"]
    )

    with dash_tab1:
        st.markdown("### ğŸ“ æ¡ˆä»¶ / æµç¨‹æ­·ç¨‹")
        history = st.session_state.get("pipeline_history", [])
        if not history:
            st.info("å°šæœªåŸ·è¡Œä»»ä½•å¯©æŸ¥æµç¨‹ã€‚")
        else:
            for run_idx, run in enumerate(reversed(history), start=1):
                with st.expander(f"æ¡ˆä»¶æµç¨‹ #{len(history) - run_idx + 1}"):
                    for step_idx, item in enumerate(run, start=1):
                        st.markdown(f"**æ­¥é©Ÿ {step_idx}** â€“ ä»£ç† `{item['agent_id']}`")
                        st.markdown(item["output"][:300] + "...")

    with dash_tab2:
        st.markdown("### ğŸ“‘ å®Œæ•´æ´»å‹•ç´€éŒ„")
        if st.session_state.combat_log:
            for entry in reversed(st.session_state.combat_log):
                st.markdown(f"{entry['icon']} {entry['message']}")
        else:
            st.info("å°šç„¡æ´»å‹•ç´€éŒ„ã€‚")

    with dash_tab3:
        st.markdown("### ğŸ… å¯©æŸ¥é‡Œç¨‹ç¢‘")

        achievements = []
        if st.session_state.player_level >= 5:
            achievements.append("ğŸ–ï¸ é€²éšå¯©æŸ¥å®˜ï¼šå¯©æŸ¥æˆç†Ÿåº¦ç­‰ç´šé” 5ã€‚")
        if st.session_state.quests_completed >= 10:
            achievements.append("ğŸ“œ æ¡ˆä»¶é”äººï¼šå®Œæˆ 10 ä»¶ä»¥ä¸Šæ¡ˆä»¶æµç¨‹ã€‚")
        if len(st.session_state.combat_log) >= 50:
            achievements.append("ğŸ“ˆ é«˜åº¦äº’å‹•ï¼šå·²åŸ·è¡Œè¶…é 50 æ¬¡æ¨¡å‹å‘¼å«æˆ–æ“ä½œã€‚")
        if st.session_state.player_level >= 10:
            achievements.append("ğŸ‘‘ è³‡æ·±å¯©æŸ¥æ¶æ§‹å¸«ï¼šå¯©æŸ¥æˆç†Ÿåº¦ç­‰ç´šé” 10ã€‚")

        if achievements:
            for ach in achievements:
                st.success(ach)
        else:
            st.info("æŒçºŒç´¯ç©æ¡ˆä»¶èˆ‡æµç¨‹ï¼Œå¯è§£é–æ›´å¤šå¯©æŸ¥é‡Œç¨‹ç¢‘ã€‚")

    with dash_tab4:
        st.markdown("### ğŸ“ˆ äº’å‹•åˆ†æåœ–ï¼ˆä»£ç†ä½¿ç”¨åˆ†ä½ˆï¼‰")
        history = st.session_state.get("pipeline_history", [])
        if not history:
            st.info("å°šç„¡æµç¨‹åŸ·è¡Œè¨˜éŒ„ï¼Œç„¡æ³•ç¹ªè£½çµ±è¨ˆã€‚")
        else:
            # Count how many times each agent_id appears
            from collections import Counter
            counter = Counter()
            for run in history:
                for step in run:
                    counter[step["agent_id"]] += 1
            data = [{"agent_id": k, "count": v} for k, v in counter.items()]
            chart = (
                alt.Chart(alt.Data(values=data))
                .mark_bar(cornerRadiusTopLeft=6, cornerRadiusTopRight=6)
                .encode(
                    x=alt.X("agent_id:N", title="Agent ID"),
                    y=alt.Y("count:Q", title="ä½¿ç”¨æ¬¡æ•¸"),
                    tooltip=["agent_id", "count"],
                    color=alt.Color("count:Q", scale=alt.Scale(scheme="blues")),
                )
                .properties(height=320)
            )
            st.altair_chart(chart, use_container_width=True)

# -----------------------------------------------------------
# Main Entry Point
# -----------------------------------------------------------

def main():
    """Main application entry point"""
    st.set_page_config(
        page_title="FDA 510(k) Multi-Agent Review Studio",
        page_icon="ğŸ¥",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    init_session_state()
    apply_custom_css()
    config = load_agents_config()
    render_enhanced_sidebar(config)

    st.markdown(f"# ğŸ¥ {get_translation('title')}")
    st.markdown(f"_{get_translation('subtitle')}_")

    render_review_context_selector()

    st.markdown("---")

    tab_input, tab_pipeline, tab_smart, tab_notes, tab_ocr, tab_dashboard = st.tabs([
        f"ğŸ“ {get_translation('input')}",
        f"ğŸ”„ {get_translation('pipeline')}",
        f"âœ¨ {get_translation('smart_replace')}",
        f"ğŸ“” {get_translation('notes')}",
        f"ğŸ“‚ {get_translation('ocr')}",
        f"ğŸ“Š {get_translation('dashboard')}",
    ])

    with tab_input:
        render_input_tab()

    with tab_pipeline:
        render_pipeline_tab(config)

    with tab_smart:
        render_smart_replace_tab()

    with tab_notes:
        render_notes_tab()

    with tab_ocr:
        render_submission_ocr_tab()

    with tab_dashboard:
        render_dashboard_tab()


if __name__ == "__main__":
    main()